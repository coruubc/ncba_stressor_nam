---
title: 'Process IUCN spp shapes to Mollweide'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

library(raster)
# library(rgeos)
source('https://raw.githubusercontent.com/oharac/src/master/R/common.R')  ###
  ### includes library(tidyverse); library(stringr); dir_M points to ohi directory
library(sf)

source(here('common_fxns.R'))
source(here('_setup/rasterize_fxns.R'))

dir_bli <- file.path(dir_M, 'git-annex/globalprep/_raw_data/birdlife_intl/d2019')
dir_shp <- file.path(dir_M, 'git-annex/globalprep/_raw_data/iucn_spp/d2020-1')
  ### These are shapefiles directly from IUCN as individual species map files
  ### (in the unzipped folder).
  ### in this folder are shapefiles at a taxonomic level.

```

# Summary

Using a set of IUCN species range maps, rasterize each species to 10 km x 10 km raster using `fasterize`.  Use `presence` field from shapefile.

* Subpopulation polygons must be identified and rasterized separately from the parent polygon; this must be done by sciname and subpop fields since the polygon IDs are based upon the parent ID.
* Regional assessments need not be determined at this stage - the ID numbers match the global ID numbers (including subpops).

# Data sources

* IUCN species shapefiles:  IUCN. (2020). The IUCN Red List of Threatened Species. Version 2020-1. Retrieved April 2020, from http://www.iucnredlist.org
* BirdLife International shapefiles: BirdLife International and Handbook of the Birds of the World. (2018). Bird species distribution maps of the world. Version 7.0. Available at http://datazone.birdlife.org/species/requestdis
* Bathymetry (GEBCO): Sandwell, D. T., Gille, S. T., & Smith, W. H. F. (2002, June). Bathymetry from Space:Oceanography, Geophysics, and Climate. Retrieved from https://www.gebco.net/


# Methods

## Read spp shapes, correct subpop IDs, `fasterize()`, depth clip, save to csv

We will loop over each species in each shapefile and rasterize separately, using `sf` and `fasterize` packages.  

* From the full map list, filter to a single shapefile
* Load shapefile using `st_read`, and correct subpop IDs from `shp_iucn_sid` to `iucn_sid`
* Loop over each `iucn_sid` in the shapefile, rasterizing (`fasterize()`) to 10 km^2 resolution, using "presence" field. 
    * clip to neritic (<=200 m) and shallow (<=60 m) depth raster if appropriate.  Otherwise mask to bathy raster.  Since bathy raster was created by masking to area raster, cells with any marine presence will be kept but any non-marine cells will be dropped.
    * Save as .tif and .csv, and compare average file sizes.  .csv easier to work with, but might be significantly larger than .tifs in the long run.
        * note: no longer saving as .tif for speed and file size - just use .csv instead!
    * use `mclapply()` to speed this up.
    
``` {r set up list of maps to rasterize}

reload <- FALSE

maps_to_rasterize <- read_csv(
  file.path(dir_data, sprintf('spp_marine_maps_%s.csv', api_version)),
  col_types = cols('subpop' = 'c')) %>%
  mutate(shp_file = str_replace(dbf_file, 'dbf$', 'shp'))

### rast_base for cell IDs
rast_base <- raster(file.path(dir_spatial, 'cell_id_mol.tif'))

### directory for mollweide rasters
dir_mol_rast <- file.path(dir_bd_anx, 'spp_rasts_mol_2020')
if(!dir.exists(dir_mol_rast)) dir.create(dir_mol_rast)

### If some species already processed, remove from the list to process.
if(reload == FALSE) {
  maps_already_rasterized <- list.files(dir_mol_rast,
                                        pattern = '.csv') %>%
    str_extract('[0-9]+') %>%
    as.integer()

  maps_to_rasterize <- maps_to_rasterize %>%
    filter(!iucn_sid %in% maps_already_rasterized)
}

### for selectively rerunning taxonomic groups
# maps_to_rasterize <- maps_to_rasterize %>%
#   filter(str_detect(dbf_file, 'REPTILES'))
# 
# asdf <- paste0('iucn_sid_', maps_to_rasterize$iucn_sid, '.csv')
# zxcv <- list.files(dir_mol_rast, 
#                    full.names = TRUE, 
#                    pattern = paste0(asdf, collapse = '|'))
# zzz <- file.mtime(zxcv)
# tmp_df <- data.frame(f = zxcv,
#                      t = zzz)
# 2018-09-12 12:38:31 check to make sure they're all later than this!
```

``` {r rasterize and clip and save to csv}

if(nrow(maps_to_rasterize) == 0) { ### all maps accounted for as .csvs
  
  message('reload == ', reload, '... No maps to process...')
  
} else {
  
  message('reload == ', reload, '... Maps to process: ', nrow(maps_to_rasterize))

  ### These will be used as masks
  rast_bathy <- raster(file.path(dir_spatial,
                                 'bathy_mol.tif'))
  rast_neritic <- raster(file.path(dir_spatial,
                                 'bathy_mol_neritic.tif'))
  rast_shallow <- raster(file.path(dir_spatial,
                                 'bathy_mol_shallow.tif'))
  
  ################################################################.
  ### Loop over each distinct shapefile with species range maps
  ################################################################.
  shps <- maps_to_rasterize$shp_file %>% unique()
  for(i in seq_along(shps)) {
    ### i <- 1
    
    shp <- shps[i]
    
    maps_in_shp <- maps_to_rasterize %>%
      filter(shp_file == shp)
    
    id_fix <- maps_in_shp %>%
      select(shp_iucn_sid, iucn_sid, subpop, max_depth) %>%
      distinct()
    
    message(i, ' of ', length(shps), ': reading ', basename(shp), ' from: \n  ', shp)

    polys_all <- read_sf(shp, type = 6) %>%
      janitor::clean_names()
      ### we will check geoms and fix them inside the mclapply, 
      ### and then reproject, one species at a time

    if(!'sciname' %in% names(polys_all)) {
      polys_all <- polys_all %>%
        rename(sciname = binomial)
    }
    
    if(!'subpop' %in% names(polys_all)) {
      polys_all$subpop <- NA_character_
      ### if shape doesn't have subpop column, add it as NA
    }
    if('id_no' %in% names(polys_all)) {
      polys_all <- polys_all %>%
        rename(iucn_sid = id_no)
    }
    if(!'presence' %in% names(polys_all)) {
      polys_all <- polys_all %>%
        mutate(presence = 1)
    }

    
    if(str_detect(shp, 'REPTILES')) {
      message('Replacing REPTILES with buffered turtle polygons!!!')
      ### the turtle polys have issues with the western boundary - not quite at
      ### +180; some have issues on the east as well. Buffer problem spp by
      ### 0.25 degrees before clipping.  Near the equator this adds an error of
      ### ~ 28 km!  But clearly the shitty polygons are creating an error as well.
      ### Identify the problem ones:
      ### Caretta caretta 3897
      ### Dermochelys coriacea 6494
      ### Eretmochelys imbricata 8005
      ### Chelonia mydas 4615
      ### I'll just buffer all subpops, for ease.  Land will get clipped
      ### later.
      turtles_buffered_file <- file.path(dir_bd_anx, 'tmp/reptiles_shp_buffered.shp')
      if(!file.exists(turtles_buffered_file)) {
        polys_match_buff <- polys_match %>%
          filter(shp_iucn_sid %in% c(3897, 6494, 8005, 4615)) %>%
          st_buffer(dist = 0.25)
        polys_match_non_buff <- polys_match %>%
          filter(!shp_iucn_sid %in% c(3897, 6494, 8005, 4615))
        polys_match <- rbind(polys_match_non_buff, polys_match_buff)
        st_write(polys_match, turtles_buffered_file)
      }
      # dbf <- foreign::read.dbf(turtles_buffered_file)
      # foreign::write.dbf(dbf, turtles_buffered_file)

      polys_match <- st_read(turtles_buffered_file) %>%
        setNames(c("shp_iucn_sid", "sciname", "subpop", "presence",
                   "iucn_sid", "max_depth", "geometry"))
    } else {
      
      polys_match <- polys_all %>%
        select(shp_iucn_sid = iucn_sid, sciname, subpop, presence, geometry) %>%
        mutate(presence = ifelse(presence == 0, 1, presence),
               subpop   = as.character(subpop)) %>%
        inner_join(id_fix, by = c('shp_iucn_sid', 'subpop')) 

    }
    
    spp_ids <- maps_in_shp$iucn_sid %>% 
      sort() %>% 
      unique()
    
  
    ####################################################################.
    ### In each shapefile, loop over each species ID using mclapply().
    ####################################################################.
    
    message('Processing ', basename(shp), ' with ', length(spp_ids), ' species...')
    
    # system.time({
      tmp <- parallel::mclapply(seq_along(spp_ids),
                                mc.cores = 24, 
                                FUN = function(x) {
        ### x <- 1
        ### spp <- 3897                          
        spp <- spp_ids[x]
        
        message(x, ' of ', length(spp_ids), ': Processing ', spp, ' in ', 
                basename(shp), ' (group ', i, ' of ', length(shps), ')...\n')
        
        spp_shp <- polys_match %>%
          filter(iucn_sid == spp)
        
        spp_shp <- valid_check(spp_shp)
          ### if invalid geom, and bounds exceeded, buffer to 0
        
        spp_shp <- spp_shp %>%
          clip_to_globe() %>%
          smoothr::densify(max_distance = 0.5) %>%
          st_transform(crs(rast_base))
        
        spp_rast <- fasterize::fasterize(spp_shp, rast_base, 
                                         field = 'presence', fun = 'min')
        
        ### depth clip if necessary; otherwise clip to bathy raster (which previously
        ### was clipped to area raster - so cells with any marine area will be kept,
        ### and non-marine cells will be dropped).
        ### Manual adds of shallow spp:
        spp_shallow <- c(133512)
        
        if(spp_shp$max_depth == '< 20 m') {
          spp_rast <- mask(spp_rast, rast_shallow)
        } else if(spp_shp$max_depth == '< 200 m' | spp_shp$iucn_sid %in% spp_shallow) {
          spp_rast <- mask(spp_rast, rast_neritic)
        } else {
          spp_rast <- mask(spp_rast, rast_bathy)
        }
        
        ### convert to dataframe and write out as a csv:
        spp_present <- data.frame(cell_id  = values(rast_base),
                                  presence = values(spp_rast)) %>%
          filter(!is.na(presence))
        
        if(nrow(spp_present) == 0) {
          message('Species ID ', spp, ' resulted in a zero-length dataframe.')
        }
        
        write_csv(spp_present, file.path(dir_mol_rast,
                                         sprintf('iucn_sid_%s.csv', spp)))
        
        return(NULL)
      }) ### end of mclapply FUN definition
    # }) ### end of system.time call
  } ### end of for loop over each species group
} ### end of "if" check to make sure there are any maps to rasterize

```

``` {r file size testing}

maps <- read_csv(
    file.path(dir_data, sprintf('spp_marine_maps_%s.csv', api_version)),
    col_types = cols('subpop' = 'c')
  ) %>%
  select(spp_group = dbf_file, iucn_sid, sciname, subpop) %>%
  mutate(spp_group = str_replace(spp_group, '.+_raw_data/|.dbf', '')) %>%
  distinct()

csvs <- list.files(dir_mol_rast) %>%
  str_extract('[0-9]+')

x <- list.files(dir_mol_rast,
                full.names = TRUE)
x <- x[str_detect(x, paste0(csvs, collapse = '|'))]

y <- data.frame(size = file.size(x)) %>%
  mutate(f = x,
         iucn_sid = str_extract(basename(f), '[0-9]+'),
         iucn_sid = as.integer(iucn_sid)) %>%
  left_join(maps, by = 'iucn_sid')

z <- y %>%
  group_by(spp_group) %>%
  summarize(mean_size_kb = round(mean(size) / 1024), 
            n_spp = n()) %>%
  # mutate(ratio = tif / csv) %>%
  ungroup() %>%
  arrange(desc(mean_size_kb))

# sum(z$tif * z$n_spp) ### 8,146,818,040:  8.1 GB for 5369 spp (pre depth clip) (some dupes, oops)
# sum(z$csv * z$n_spp) ### 3,179,096,927:  3.2 GB for 5369 spp (pre depth clip) (some dupes, oops)
# sum(z$csv * z$n_spp) ### 2,218,041,470:  2.2 GB for 5332 spp (post depth clip) (dupes removed!)

zz <- y %>%
  summarize(mean_size_kb = round(mean(size) / 1024), 
            n_spp = n()) %>%
  mutate(spp_group  = 'TOTAL: all available spp maps',
         total_size_kb = round(sum(y$size) / 1024)) %>%
  bind_rows(z) %>%
  select(spp_group, n_spp, everything())

DT::datatable(zz, caption = 'Mean file size by group and type')

```


